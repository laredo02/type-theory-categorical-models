\documentclass[12pt]{book}

\usepackage{stmaryrd}  % for \mapstochar
\usepackage{bussproofs} % inference rules
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{mathpartir} % for inference rules
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta} % REQUIRED for 'right=of' and arrow styles

\newcommand{\subst}[2]{[#1 := #2]}    % Substitution
\newcommand{\step}{\curly} % One-step beta reduction
\newcommand{\lam}[2]{\lambda #1.\,#2} % Lambda abstraction

\newcommand{\app}[2]{#1.\,#2} % Lambda abstraction
\newcommand{\abs}[2]{\la #1.\,#2} % Lambda abstraction
\newcommand{\curlysym}{\mathrel{\leftrightarrow}_\beta}


\input{packages.tex}

\begin{document}

\magenta{
  The flow of the document, what will be covered (UTLC, STLC, DTLC, Curry-Howard, LCCCs...).
  The aim of the document. Things will be formal but for key constructions that are deemed to be of practical use, a simple implementation will be given, usually in haskell, lean or similar. Implementations given will not necessarly be optimal since they are meant for educational purpuse
}

\chapter{\lCalc}

In the early 20th century, mathematicians such as Bertrand Russell, David Hilbert, and Kurt Gödel were trying to set complete, consistent, and decidable foundations for mathematics. Within this context, Alonzo Church, in he's effort to formalize the notion of effective computability, he introduced a minimal symbolic language based upon function abstraction and function application, this system is now called \lcalc.

In 1936, Church used lambda calculus to address Hilbert’s Entscheidungsproblem—whether a mechanical method could determine the truth of any first-order logic statement. He proved no such algorithm exists, establishing the undecidability of first-order logic.

One of the nuances formalized by the \lcalc \ is the disctinction between extensional and intensional equality. The extensional approach to equivalence states that two functions are equivalent if they share input output pairs for every posible input. It's intensional counterpart extends this notion of equality by requiring that the procedures that compute these pairs share complexity i.e. they take the same steps toward yielding a result. \footnote{
  Let \( p \) be a sufficiently large prime, and let \( f, g : \mathbb{Z}_p \to \mathbb{Z}_p \) be defined by \( f(x) = x^2 \) and \( g(x) = \log_a(a^{x+2}) \), where \( a \in \mathbb{Z}_p^\times \) is a fixed primitive root. Although \( f \) and \( g \) are extensionally equal, i.e., they yield the same output for all inputs in \( \mathbb{Z}_p \), they are intensionally distinct. The function \( f \) performs a simple squaring operation, while \( g \) requires evaluating a discrete logarithm, intractable in general. \magenta{\ref{}[Pedro Bonilla]
}
}

Languages like LISP, Haskell, Erlang and others share the \lcalc \ as it's theoretical foundation. It has has established itself as the backbone of functional programming. The introduction of type systems into the \lcalc \ has allowed us to computer-verify mathematical proofs and develop programs that are correct by construction. \footnote{Principles of the \lcalc \ and Type Theory underlie every computer assisted verification tool as well as kitchen table programming languages like Java or C.}


\newpage
\section{\centering Untyped \lCalc}
In order to get acquainted with the \lcalc, let us develop a simple example to familiarize ourselves before we begin with a more formal approach to this discipline. Consider the function $f(x) = x + 1$, the most straightforward way to express this using \lcalc \ would be $(\la x . x + 1 )$, where the lambda denotes that $x$ is being captured and used as a parameter to perform some computation \footnote{this is an e.g., $(\la x . x + 1 )$ is not a valid lambda term, see Definition \ref{def:lambda-terms-2}.}. Evaluating $f(2)$ using this newly created lambda term would look like this: $(\la x . x + 1)(2) \curly (2 + 1) \curly 3$. This process of reducing a $\la$-expression is referred to as \bred, it will be covered formally later in the chapter.
\begin{definition} The set of lambda terms \( \La \) is defined inductively as follows:
  \label{def:lambda-terms-1}
  \begin{itemize}
  \item (Variable) If \( x \in \Vset \), then \( x \in \La \).  
  \item (Abstraction) If \( x \in \Vset \) and \( M \in \La \), then \( (\la x. M) \in \La \).
  \item (Application) If \( M, N \in \La \), then \((M N) \in \La \).
  \end{itemize}
  Where $\Vset = \{x, y, z, ... \}$ represents a countably infinite set of variable names.
\end{definition}
The key takeaway of this definition is that abstraction and application together, encapsulate the meaning of function in a way that when combined with \bred \ \ref{} allows us to perform computation.

Since we are dealing with a formal language, it is in our benefit to introduce a few other objects with the aim of defining a grammar to generate the set $\La$:
\begin{itemize}
\item An alphabet \( \Sigma = \{ \la, ., (, ), \ldots \} \), is a finite set of symbols
\item A string is a finite sequence of elements from \( \Sigma \), the empty string is denoted by \( \varepsilon \)
\item \( \Sigma^* \) denotes the set of all finite strings over \( \Sigma \), \( \varepsilon \in \Sigma^* \)
\item A language \( L \) over an alphabet \( \Sigma \) is a subset of \( \Sigma^* \)
\end{itemize}
Our aim now, to generate the set $\La$,  to do this, we will make use of a grammar. When dealing with grammars that define programming languages i.e. context-free grammars, Backus-Naur Form is the way to go:
\begin{itemize}
\item Nonterminals are enclosed in angle brackets (e.g. \texttt{<expr>})
\item Terminals are written literally (e.g. \texttt{"$\lambda$"}, \texttt{"."}, $x$)
\item Productions define how nonterminals expand, written as \texttt{::=}
\item The vertical bar \texttt{|} denotes available expansions
\end{itemize}
Thus, to define the language for natural numbers in decimal notation:
\begin{align*}
  \texttt{<digit>} &\;\texttt{::=}\; \texttt{"0"} \;|\; \texttt{"1"} \;|\; \texttt{"2"} \;|\; \texttt{"3"} \;|\; \texttt{"4"} \;|\; \texttt{"5"} \;|\; \texttt{"6"} \;|\; \texttt{"7"} \;|\; \texttt{"8"} \;|\; \texttt{"9"} \\
  \texttt{<number>} &\;\texttt{::=}\; \texttt{<digit>} \;|\; \texttt{<digit>} \ \texttt{<number>}
\end{align*}
\begin{definition} Taking advantage of BNF notation, an alternative definition for $\La$ (\ref{def:lambda-terms-1}):
  \begin{align*}
    \texttt{<term>} &\;\texttt{::=}\; \texttt{<variable>} \\
                    &\;|\; \texttt{"$\la$"}\ \texttt{<variable>}\ \texttt{"."}\ \texttt{<term>} \\
                    &\;|\; \texttt{"("}\ \texttt{<term>}\ \texttt{<term>}\ \texttt{")"} \\
    \texttt{<variable>} &\;\texttt{::}\in V
  \end{align*}
  \label{def:lambda-terms-2}
  where \( \Sigma = \{ \la, ., (, )\} \cup \Vset \) and \( \La \subset \Sigma^* \). Through use of algebraic types \footnote{Algebraic Data Types are inductively defined types built from sums (either this or that) of products (this and that). They let you define structured data with multiple forms and support safe, exhaustive pattern matching. }, this grammar can be implemented in Haskell:
  \begin{lstlisting}[style=haskellstyle,caption={Grammar implementation of the \lcalc \ language using Haskell's algebraic types}]
data LambdaTerm = Var String
                | App LambdaTerm LambdaTerm
                | Abs String LambdaTerm
{- variable names are implemented using strings -}
  \end{lstlisting}
\end{definition}
\begin{remark}
  From the definition above we extract that \( \lambda x . x + 1 \notin \La \), given this circumstance, the need to find an accurate representation for numbers within this definition arises. See Definition \ref{def:church-naturals}
\end{remark}
\begin{example} \label{ex:lambda-terms} Some examples of valid $\la$ terms generated using the grammar in \ref{def:lambda-terms-2}:
  \( y \),
  \( (\la x. (x x)) \),
  \( (\la x. (\la y. x)) \),
  \( (((\la x. (x y)) (\la y. y)) z) \).
  The haskell counterpart for the second term is:
  \begin{lstlisting}[style=haskellstyle,caption={Haskell interpretation of the second term}]
Abs "x" (App (Var "x") (Var "x"))
  \end{lstlisting}
\end{example}
As a convention to avoid notational cluttering, the outermost parenthesis can be omitted e.g. $ ( \la x.x ) $ can be read as $ \la x.x $. Also, application is left-associative  and binds tighter than abstraction, so $M N P$ is parsed as $(M N) P$ and $\la x . M N$ means $\la x . (M N)$, not $(\la x . M) N$. Likewise, abstraction is right-associative e.g. $\la x . \la y . M$ is $\la x . (\la y . M)$ and binds more weakly than application. All these are just to keep notation straight-forward, the formal definition does not leave precedence under-specified thanks to parenthesis.
\subsection{\centering Equivalence of terms}
Having defined the language of the \lcalc, we move on to the mechanics of computation. To this end, several equivalence relations among terms will be defined. Let us begin by introducing the set that contains all variables that are not bound to the term via an abstraction.
\begin{definition} Given $T \in \La $, $\functionfont{FV} : \La \to \mathcal{P}(\Vset) $ outputs the set of free variables for $T$:
  \begin{align*}
    & \FV x = {x}\\
    & \FV {MN} = \FV M \cup \FV N \\
    & \FV {\lambda x . M} = \FV M \setminus \{x\}
  \end{align*}
  Where $ x \in \Vset $ and $ M, N \in \La $, and $\mathcal{P}$ denotes the power set.
\end{definition}
\begin{example}
  Compute the free variables of $\lambda x . \lambda y . y x z$:
  
  The term well-formed, thus belongs to $\La$, so we proceed:
  \begin{align*}
    \FV {\lambda x . \lambda y . y x z} &= \FV {\lambda y . yxz} \setminus \{x\} \\
                                        &= \FV {yxz} \setminus \{x, y\} \\
                                        &= \{x, y, z\} \setminus \{x, y\} \\
                                        &= \{z\}
  \end{align*}
  Of course, $z$ is the only free variable in the expression as it is the only variable that is not captured by some $\la$-abstraction.
  \begin{lstlisting}[style=haskellstyle,caption={asdasd}]

  \end{lstlisting}
\end{example}
\begin{definition}
  A term $M \in \La$ is closed if and only if $\FV M = \emptyset$, closed lambda terms are often referred to as combinators. $\Lambda^0$ is the set of all closed lambda terms.
\end{definition}
Since every lambda term is made up of other smaller lambda terms, it is only natural to define the set that contains all subterms of a given term:
\begin{definition} Given $T \in \La$, $\functionfont{Sub} : \La \to \mathcal{P}(\Vset)$ maps terms to the set of it's subterms.
  \begin{align*}
    &\Sub x = \{x\} \\
    &\Sub {MN} = \Sub M  \cup \Sub N \cup \{MN\} \\
    &\Sub {\la x . M} = \Sub M  \cup \{\la x. M\}
  \end{align*}
  Where $x \in \Vset$ and $M, N \in \La $.
  \begin{lstlisting}[style=haskellstyle,caption={asdasdasd}]

  \end{lstlisting}
\end{definition}
\begin{remark}
  Let \( \preceq \) be the subterm relation on \lterms, defined by
  \[
    M \preceq N \iff M \in \Sub{ N }
  \]
  \( \preceq \) is a partial order relation. This statement is follows intuitively from the definition, it can be proven using induction on the derivation tree.
\end{remark}
\begin{definition} A term $ S \in \Sub M, \ M \in \La$ is said to be proper if $S \neq M$. $\Sub{M} \setminus \{M\}$ would be the set of proper subterms.
\end{definition}
\begin{definition} Let \( M = \lambda x. N \) be a \lterm:
  \begin{itemize}
  \item The variable \( x \) is the binding variable of the abstraction \( \lambda x. N \).
  \item An occurrence of a variable \( x \) in \( M \) is bound if \( x \in \Sub { \lambda x. N } \).
  \end{itemize}
\end{definition}

\begin{definition}Capture-Avoiding Substitution Rules
  Let \( M, N \in \La \) and \( x, y, z \in \Vset \). The substitution \( M[x := N] \) is defined inductively as:
  \[
    \begin{aligned}
      x[x := N]                       & = N \\
      y[x := N]                       & = y && y \neq x \\
      (M_1\, M_2)[x := N]             & = (M_1[x := N])\,(M_2[x := N]) \\
      (\lambda x. M)[x := N]          & = \lambda x. M \\
      (\lambda y. M)[x := N]          & = \lambda y. (M[x := N]) && y \neq x, y \notin \FV { N } \\
      (\lambda y. M)[x := N]          & = \lambda z. (M[y := z][x := N]) && y \neq x, y \in \FV { N }
    \end{aligned}
  \]
  where \( z \notin \FV { M } \cup \FV { N } \).
  \begin{lstlisting}[style=haskellstyle,caption={asdasdasd}]

  \end{lstlisting}
\end{definition}
\magenta{\begin{itemize}
  \item De Bruijin
  \item Add insight and intuition upon the formal definition
  \item Add some haskell implementation details
  \item Maybe add code that generates \aequivlt \ terms
  \end{itemize}}
\begin{definition} Two lambda terms \( M, N \in \La \) are \aequivlt, written \( M =_\alpha N \), if they are structurally identical except for the names of bound variables.
  Formally:
  \[
    \lam{x}{M} =_\alpha \lam{y}{M\subst{x}{y}}
    % \quad \text{if } y \notin \FV { M } 
  \]
  Alpha-equivalence posseses the following properties:
  \[
    \begin{aligned}
      M &=_\alpha M \\
      M &=_\alpha N \Rightarrow N =_\alpha M \\ 
      M &=_\alpha N \text{ and } N =_\alpha P \Rightarrow M =_\alpha P
    \end{aligned}
  \]
  And thus, is an equivalence relation.
\end{definition}
\magenta{
  \begin{itemize}
    \item Examples \begin{example}\end{example} ... \aequiv \ relations
    \item This relation is the most horizontal of the ones that show up in the text. By this I mean it just relates terms with equivalent terms at the same level in the \bred \ tree. (find a way to say this without having introduced \bred \ yet)
  \end{itemize}
}
\begin{definition} Single-step \bred \ defined using inference rules: \orange{(remove vertical space below)}
  \begin{center}
    \[
      \begin{aligned}
        &\inferrule*[right=\textsc{Beta}]
          { }
          { (\lam{x}{M})\,N \step M\subst{x}{N} } \\[2ex]
        &\inferrule*[right=\textsc{AppL}]
          { M_1 \step M_1' }
          { M_1\,M_2 \step M_1'\,M_2 }
      \end{aligned}
      \quad
      \begin{aligned}
        &\inferrule*[right=\textsc{Abs}]
          { M \step M' }
          { \lam{x}{M} \step \lam{x}{M'} } \\[2ex]
        &\inferrule*[right=\textsc{AppR}]
          { M_2 \step M_2' }
          { M_1\,M_2 \step M_1\,M_2' }
      \end{aligned}
    \]
  \end{center}
  where \( M, N \in \La \).
  For those not so familiar with inference rules here is an alternative definition where $L$ also belongs to $\La$:
\end{definition}
\begin{itemize}
  \item \( (\lam{x}{M})\,N \step M\subst{x}{N} \)
  \item If $M \step N$, then: \( M\,L \step N\,L; L\,M \step L\,N; \lam{x}{M} \step \lam{x}{N} \)
\end{itemize}
\magenta{
  Here, The Beta rule abstracts the concept of reduction, that encapsulates the idea of computation within the \lcalc. The reason for this is that Beta reduces an application of an abstraction and removes the abstraction, hence simplifying the term. 
}

Single \bred s can be applied succesively, which induces the following definition

\begin{definition} Zero or more step \bred, or as in the literature, \emph{reflexive-transitive-closure}:

\smallskip
We write $M \step^* N$ if and only if there exists a sequence of one-step $\beta$-reductions starting from $M$ and ending at $N$. \footnote{The convention in the literature is to use $\rightarrow_\beta$ for \bred s $\twoheadrightarrow_\beta$ for multi-step \bred s. I prefer to use $\curly$ and $\curly^n$ since this allows me to use the superscript to note the number of steps in the process \bred \ e.g. $ \FALSE \, M \, N \curly^2 $ N. Instead of $\curly^n$ I use $\curly^*$.}
\[
  M \equiv M_0 \step M_1 \step M_2 \step \cdots \step M_{n-1} \step M_n \equiv N
\]
That is, there exists an integer $n \geq 0$ and a sequence of terms $M_0, M_1, \dots, M_n$ such that: $ M_0 \equiv M $ and $ M_n \equiv N $ for all $ i $ with $ 0 \leq i < n $, we have $ M_i \step M_{i+1} $.
\end{definition}

\magenta{ This process of b reducing a term leads raises the question of whether all terms can be reduced indefinitly or whether all terms have some kind of normal form that cant be reduced. (obv not since variables in V cant be reduced), so we are left with: Do all terms reduce to a final form where the only possible bred is M bred M? See recursion Y combinator...}

\begin{definition} \bequiv \ relation \magenta{add more detail and complete the definition} \\
\[
  M =_\beta N \iff M \step^* N \lor N \step^* M
\]
Put in other words:
\[
    M =_\beta N \iff \exists M_0, \dots, M_n \text{ s.t. } M \equiv M_0 \leftrightarrow_\beta M_1 \leftrightarrow_\beta \cdots \leftrightarrow_\beta M_n \equiv N
\]
where $ \leftrightarrow_\beta $ denotes a bidirectional $\curly$.
\end{definition}

Having covered \bred, we now explore a key aspect of the \lcalc, in the jargon referred to as first-class citizenship, or more formally high-order. When we use high-order to refer to a $\la$-term, we refer to the fact that both functions and arguments are treated indistiguishably, the example below sheds some light on the matter:
\begin{align*}
  \overbrace { (\lambda f . \lambda y . f f y) }^{A} \overbrace{ (\lambda x . x + 1) }^{B} \overbrace{(2)}^{C}
  &\curly (\lambda y .(\lambda x . x + 1) (\lambda x . x + 1)y)(2) \\
  &\curly (\lambda x . x + 1)(\lambda x . x + 1)(2) \\
  &\curly^* (\lambda x . x + 1) (3) \\
  &\curly^* (4)
\end{align*}

Taking a closer look, $A$, applies $f$ twice to $x$, as a consequence, $B$ is being applied twice to $C$. And so, very easily, we have implemented the $ x + 2 $ function via a sequential application of $x + 1$ to $C$. One can intuitively apreciate the computational expresivenes this brings with it, and how the sintactic-semantic homogeneity sets the \lcalc \ apart from the classical set-theoretic approach to functions.

\magenta{normalization + church rosser i.e. confluence}











\subsection{\centering Some Important Constructs}

Within the \lcalc \ there exist a number of conventional constructions that represent key ideas in programming (numbers, booleans, branches and lists among others) In the following we define a few of the most important ones.

We begin defining $ \mathbb{N} $ for no particular reason \orange{other than the natural numbers are the foundation of most of mathematics}. The notion of natural numbers in the \lcalc \ follows from the Peano axioms:
\begin{itemize}
\item (Zero) \( 0 \in \NAT \)
\item (Successor) \( \forall x \in \NAT, s(x) \in \NAT \)
\item (Initial) \( \forall x \in \NAT, s(x) \neq 0 \) \label{def:peano-nat-initial}
\item (Injectivity) \( \forall x, y \in \NAT, s(x) = s(y) \Rightarrow x = y \)
\item (Induction) \( P(0) \land \forall ( P(x) \Rightarrow P(s(x))) \Rightarrow \forall x,  P (x) \)
\end{itemize}
\magenta{Maybe include here a graph to make the intuition behind the axioms}
Say for instance we mean to represent $ 3 $ using Peano notation, we would come up with $ s(s(s(0))) $. To add two numbers one can just use the set of rules:
\[
  \begin{cases}
    n + s(m) = s(n + m) \\
    n + 0 = n
  \end{cases}
\]
A recursive rule that performs the addition, and a base case to stop the recursion for n + 0, since there is no such number that has $ 0 $ as succesor \ref{def:peano-nat-initial}.
\begin{definition} Representation of the set of all naturals, usually referred to as Church numerals:
  \label{def:church-naturals}
  \begin{align*}
    \texttt{0} &\equiv \la f.\la x. x \\
    \texttt{SUCC} &\equiv \la n. \la f. \la x . f ( n f x )
  \end{align*}
\end{definition}
\begin{remark}
  The church numerals have been defined similarly to the peano axioms, that is, by bringing into existance an initial element and using the succesor function to define the rest of them. Let us apply \texttt{SUCC} a few times to illustrate:
  \begin{align*}
    \texttt{0} &\equiv \la f.\la x. x \\
    \texttt{1} &\equiv \la f.\la x. f x \\
    \texttt{2} &\equiv \la f.\la x. f f x \\
    \texttt{3} &\equiv \la f.\la x. f f f x \\
    \texttt{n} &\equiv \la f.\la x. f ^ n x
  \end{align*}
  where $ n $ denotes the number of times we append $ f $.
\end{remark}
Now that we have a construction for the naturals, we can define some arithmetic operations:
\begin{align*}
  \texttt{ADD} &\equiv \la m.\la n. \la f. \la x. m f ( n f x) \\
  \texttt{MUL} &\equiv \la f.\la x. \la f. \la x. m (n f) x \\
  \texttt{ISZERO} &\equiv \lambda n. n\,(\lambda x.\texttt{FALSE})\,\texttt{TRUE}
\end{align*}
This are left as a sidequest since they will not be used in the rest of the text.

The next construction in line are the truth values of boolean logic, ``\textit{true}'' and ``\textit{false}'' for short.
\begin{definition} Boolean truth values:
  \begin{align*}
    \texttt{TRUE} &\equiv \lambda t.\lambda f. t \\
    \texttt{FALSE} &\equiv \lambda t.\lambda f. f
  \end{align*}
\end{definition}
\begin{remark}
  \label{rem:true-false}
  Note how $\TRUE \ M \ N \curly M $ and $ \FALSE \ M \ N \curly N $
\end{remark}
\begin{definition} Conditional, in other words \textit{``if statement''}.
  \begin{align*}
    \texttt{IF} &\equiv \lambda b.\lambda t.\lambda e. b\,t\,e
  \end{align*}
\end{definition}
The reality of the \IF \ is that it is just a combinator that happens to behave as a branching operator whenever we pass a church encoded boolean as a first argument.
\begin{example} On the correct behabiour of $\IF$ i.e. check that it implements branching. $ M, N \in \La $ and $ \IF, \TRUE, \FALSE$ are as usual:
  \[
    \begin{aligned}
      & \IF \ \TRUE \ M \ N \\
      \equiv \ & (\la b.\la t.\la e.\, b\, t\, e) \ \TRUE \ M \ N  \\
      \curly \ & (\la t.\la e.\, \TRUE\ t\, e) \ M \ N  \\
      \curly \ & (\la e.\, \TRUE\ M\, e) \ N  \\
      \curly \ & \TRUE\ M\, N \curly^* M 
    \end{aligned}
    \quad
    \begin{aligned}
      & \IF \ \FALSE \ M \ N \\
      \curly^* \ & \FALSE\ M\, N \curly N
    \end{aligned}
  \]
  So the \IF \ just combines $b, t, e$ in such a way that whenever b is a church encoded boolean either $t$ or $e$ is dropped depending on the truth value of $b$. See Remark \ref{rem:true-false}. From this example we extract that $ \la x . x $ is equivalent to \IF \ it terms of behaviour. Thus, we could use $ \IF \equiv \la x . x $ interchangeably.
\end{example}
\begin{remark}
  Using the \IF \ we can implement an universal set of logic gates:
  \begin{align*}
    \NOT &\equiv \IF \, b \, \FALSE \, \TRUE \\
    \AND &\equiv \IF \, a \, (\IF \, b \, \TRUE \, \FALSE) \, \FALSE
  \end{align*}
  This allows us to simulate a nand gate, known to be universal \orange{\ref{} universality, conjunctive normal form}. Thus, the lambda calculus is at least equivalent to propositional logic \orange{\ref{} Something on turing completeness}.
\end{remark}
On the same track, another must-have for programmers is lists. Sice they are a bit more involved, we require tuples to define them. Note that the \IF \ is a tuple that contains two elements, we query the first element using a \TRUE \ i.e. the \textit{``then''} branch, and the sencond using a \FALSE \ i.e. the \textit{``else''} branch. We will be using this alternative definition for convenience:
\begin{definition} \( \PAIR \equiv \la x.\la y.\la f. f\,x\,y \)
\end{definition}
\begin{remark}
  This is more convenient since $ \PAIR \, M \, N \curly^* \la f . f \, M \, N  $, which allows us to query the first element using $ (\la f . f \, M \, N) \, \TRUE $ and the second by $ (\la f . f \, M \, N) \, \FALSE $, this motivates the definition below:
\end{remark}
\begin{definition}
\begin{align*}
  \FST &\equiv \la p. p\,(\la x.\la y. x) \\
  \SND &\equiv \la p. p\,(\la x.\la y. y)
\end{align*}
\end{definition}
\begin{proposition} A key property of $ \PAIR $'s is:
  \[\PAIR \, A \, B \equiv_\beta \PAIR \, (\FST \, A \, B) \, (\SND \, A \, B) \]
\end{proposition}
Using pairs, we now can define linear lists, we can achieve this using either left linear trees or right linear trees:
\begin{center}
  \[
    \begin{aligned}
      \begin{forest}
        [\rotatebox{60}{\texttt{\ldots}}
        [\PAIR
        [\PAIR
        [\NIL]
        [\texttt{ELEM}]
        ]
        [\texttt{ELEM}]
        ]
        [\texttt{ELEM}]
        ]
      \end{forest}
    \end{aligned}
    \hspace{1cm}
    \textit{or}
    \hspace{1cm}
    \begin{aligned}
      \begin{forest}
        [\rotatebox{-60}{\texttt{\ldots}}
        [\texttt{ELEM}]
        [\PAIR
        [\texttt{ELEM}]
        [\PAIR
        [\texttt{ELEM}]
        [\NIL]
        ]]]
      \end{forest}
    \end{aligned}
  \]
\end{center}
\begin{definition} We define \NIL \ in the same way we defined \texttt{0} and \FALSE \ before: $ \NIL \equiv \la f.\la z. z $; \NIL \ represents the terminating element of the inductive definition for lists. To build lists, we cons lists together using:
  \[ \texttt{CONS} \equiv \la h.\la t.\la f.\la z. f\,h\,(t\,f\,z) \]
that binds a new element $h$ to to an pre-existing list or to \NIL.
\end{definition}
\begin{note}
  This notion of inductively defined lists is lays the foundations of the \textbf{LISP} family of programming languages, hence the name (LISt Processing).
\end{note}
\begin{definition} Relevant list operators:
\begin{align*}
  \texttt{ISNIL} &\equiv \la l. l\,(\la h.\la t. \FALSE)\,\TRUE \\
  \texttt{HEAD} &\equiv \la l. l\,(\la h.\la t. h)\,\text{undef}
\end{align*}  
\end{definition}























































\newpage
\centering{\blue{left blank}}



\begin{align*}
  \texttt{CONS} &\equiv \la h.\la t.\la f.\la z. f\,h\,(t\,f\,z) \\
  \texttt{IS\_NIL} &\equiv \la l. l\,(\la h.\la t. \texttt{FALSE})\,\texttt{TRUE} \\
  \texttt{HEAD} &\equiv \la l. l\,(\la h.\la t. h)\,\text{undef} \\
  \texttt{TAIL} &\equiv \la l.\, \texttt{FIRST}\,(l\,(\la p.\la h.\, \texttt{PAIR}\,(\texttt{SECOND}\,p)\,(\texttt{CONS}\,h\,(\texttt{SECOND}\,p)))\,(\texttt{PAIR}\,\texttt{NIL}\,\texttt{NIL}))
\end{align*}





\textbf{Recursion:}
\begin{align*}
  \texttt{Y} &\equiv \lambda f.(\lambda x. f\,(x\,x))\,(\lambda x. f\,(x\,x))
\end{align*}
\subsection{\centering Fixed Point Combinators and Recursion}
Y combinator.
\[
  Y \triangleq \lambda f.\, (\lambda x.\, f\, (x\, x))\, (\lambda x.\, f\, (x\, x))
\]
Turing combinator
\[
  \Theta \triangleq 
  (\lambda x\, f.\, f\, (x\, x\, f))\, (\lambda x\, f.\, f\, (x\, x\, f))
\]
Z combinator
\[
  Z \triangleq \lambda f.\, 
  (\lambda x.\, f\, (\lambda v.\, x\, x\, v))\, 
  (\lambda x.\, f\, (\lambda v.\, x\, x\, v))
\]
\magenta{Turing completeness of the \lcalc} 
\section{\centering Simply Typed \lCalc}
\magenta{rewrite this pls $==>$}

The Untyped Lambda Calculus in computationally equivalent to a  Turing machine. However, with great computational power comes limited decidability of properties, leading to non-termination, or expressions such as $x(\lambda y . y)$, whose meaning is unclear.

A classic example of non-termination is the $Y$ combinator, however, the Simply Typed Lambda Calculus does not allow such expressions, as its type system is unable to assign a valid type to them.

\magenta{if there is a proof of turing completeness, add how if we remove combinators then we remove turing completeness i.e. finite tape $\neq$ turing complete}

To understand why, consider the role of function types: in the world of functions, a function maps values from a domain to a range. The Simply Typed Lambda Calculus enforces this structure explicitly, ensuring that every function application is well-typed and preventing self-application patterns that would lead to paradoxes or infinite loops.

Having grasped the untyped lambda calculus's Turing completeness and ability to compute all computable functions, we now seek properties related to decidability. To this end, we introduce the simply typed lambda calculus. Although it possesses less computational power than its untyped counterpart, it offers attractive features regarding decidability that will be useful later on.
\chapter{The Curry-Howard Correspondence}
\section*{\centering A Primer on Logic}
\magenta{build up to first order logic}
\end{document}


